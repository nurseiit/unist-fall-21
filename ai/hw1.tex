\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[latin1]{inputenc}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\title{AI Assignment 1} 
\author{Nurseiit Abdimomyn -- 20172001}
\date{26/09/2021}

\begin{document}
\maketitle

\begin{enumerate}
  \item[1.1]
  We have $\theta = [\theta_0, \theta_1, \theta_2]^T$ and $x_i = [1, x_{i0}, x_{i1}]^T$
  for $h_\theta(x) = \theta^T x$.

  So we derive for the gradient:

  $\frac{\partial L(\theta)}{\partial \theta_0} = 
  \frac{\partial}{\partial \theta_0} \sum\limits_{i} (\theta_0 + \theta_1 x_{i0} + \theta_2 x_{i1} - y_i)^2$

  $= \sum\limits_{i} \frac{\partial}{\partial \theta_0} (\theta_0 + \theta_1 x_{i0} + \theta_2 x_{i1} - y_i)^2$
  
  $= \sum\limits_{i} 2 (\theta_0 + \theta_1 x_{i0} + \theta_2 x_{i1} - y_i)$

  Doing the same for $\theta_1$ and $\theta_2$ we get:

  $\frac{\partial L(\theta)}{\partial \theta_1} = \sum\limits_{i} 2 (\theta_0 + \theta_1 x_{i0} + \theta_2 x_{i1} - y_i) * x_{i0}$
  
  $\frac{\partial L(\theta)}{\partial \theta_2} = \sum\limits_{i} 2 (\theta_0 + \theta_1 x_{i0} + \theta_2 x_{i1} - y_i) * x_{i1}$


  To find a minimum by gradient descent we use:

  $\theta^{k + 1} \gets \theta^k - \eta_k \frac{\partial L(\theta)}{\partial \theta}$ where we have $\eta = 0.01$ for all $k$.

  Thus, $\theta^{k + 1} \gets \theta^k - 0.01 * \frac{\partial L(\theta)}{\partial \theta}$. \\ \\


  Initial parameter $\theta^0 = [0, 0, 0]^T$, so compute for $\theta^1$ simultaneously:

  $\theta^1_0 = \theta^0_0 - 0.01 * \sum\limits_{i} 2 (\theta^0_0 + \theta^0_1 x_{i0} + \theta^0_2 x_{i1} - y_i)$

  $= 0 - 0.01 * \sum\limits_{i} 2 (0 + 0 * x_{i0} + 0 * x_{i1} - y_i)$
  
  $= 0.02 * \sum\limits_{i} y_i = 0.02 * 37 = 0.74$. \\ \\


  $\theta^1_1 = \theta^0_1 - 0.01 * \sum\limits_{i} 2 (\theta^0_0 + \theta^0_1 x_{i0} + \theta^0_2 x_{i1} - y_i) * x_{i0}$

  $= 0 - 0.01 * \sum\limits_{i} 2 (0 + 0 * x_{i0} + 0 * x_{i1} - y_i) * x_{i0}$
  
  $= 0.02 * \sum\limits_{i} (y_i * x_{i0})$

  $= 0.02 * (6 * 1 + 4 * 2 + 7 * 3 + 10 * 3 + 10 * 4) = 0.02 * 105 = 2.1$. \\ \\

  $\theta^1_2 = \theta^0_2 - 0.01 * \sum\limits_{i} 2 (\theta^0_0 + \theta^0_1 x_{i0} + \theta^0_2 x_{i1} - y_i) * x_{i1}$

  $ = 0 - 0.01 * \sum\limits_{i} 2 (0 + 0 * x_{i0} + 0 * x_{i1} - y_i) * x_{i1}$
  
  $ = 0.02 * \sum\limits_{i} (y_i * x_{i1})$

  $ = 0.02 * (6 * 2 + 4 * 1 + 7 * 2 + 10 * 3 + 10 * 5) = 0.02 * 110 = 2.2$.

  So for the first iteration we have $\theta^1 = [0.74, 2.1, 2.2]^T$. \\ \\

  Second iteration:

  $\theta^2_0 = \theta^1_0 - 0.01 * \sum\limits_{i} 2 (\theta^1_0 + \theta^1_1 x_{i0} + \theta^1_2 x_{i1} - y_i)$

  $= 0.74 - 0.01 * \sum\limits_{i} 2 (0.74 + 2.1 x_{i0} + 2.2 x_{i1} - y_i)$
  
  $= 0.74 - 0.02 * 22.6 = 0.288$. \\ \\

  $\theta^2_1 = \theta^1_1 - 0.01 * \sum\limits_{i} 2 (\theta^1_0 + \theta^1_1 x_{i0} + \theta^1_2 x_{i1} - y_i) * x_{i0}$
  
  $= 2.1 - 0.01 * \sum\limits_{i} 2 (0.74 + 2.1 x_{i0} + 2.2 x_{i1} - y_i) * x_{i0}$
  
  $= 2.1 - 0.02 * \sum\limits_{i} (0.74 + 2.1 x_{i0} + 2.2 x_{i1} - y_i) * x_{i0} = 0.6536$ \\ \\

  $\theta^2_2 = \theta^1_2 - 0.01 * \sum\limits_{i} 2 (\theta^1_0 + \theta^1_1 x_{i0} + \theta^1_2 x_{i1} - y_i) * x_{i1}$

  $ = \theta^1_2 - 0.02 * \sum\limits_{i} (\theta^1_0 + \theta^1_1 x_{i0} + \theta^1_2 x_{i1} - y_i) * x_{i1}$
  
  $ = 2.2 - 0.02 * \sum\limits_{i} (0.74 + 2.1 x_{i0} + 2.2 x_{i1} - y_i) * x_{i1} = 0.6776$. \\ \\

  Thus, $\theta^2 = [0.288, 0.6536, 0.6776]^T$.

\end{enumerate}

\end{document}
